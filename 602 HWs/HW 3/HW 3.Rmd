---
title: "DATA 602 HW 3"
author: "Kane Smith"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: yes
  html_document:
    toc: yes
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, include=FALSE}
options(tinytex.verbose = TRUE)
```

```{r analysis, include=FALSE}
library(ggplot2)
library(dplyr)
library(mosaic)
library(resampledata)
```

**Setting seed so that my explanations match my outputs from bootstrapping and simulations using random sampling:**

```{r}
set.seed(555)
```


## **Question 1**

### **a)**
```{r}
smoker_no <- subset(NCBirths2004, Smoker == "No")
smoker_yes <- subset(NCBirths2004, Smoker == "Yes")
head(NCBirths2004)

```

We will create 2 vectors, one with the baby weights of non smokers and one with the baby weights of smokers. We then sample from vectors these to calculate our sample means for each category respectively.

```{r}
smoker_vec <- smoker_yes$Weight
non_smoker_vec <- smoker_no$Weight
q1a_bootstrap = do(1000)*(mean(resample(non_smoker_vec)) - mean(resample(smoker_vec)))
```

### **b)**
```{r}
qdata(~result, c(0.025, 0.975), data=q1a_bootstrap)
```

Our 95% confidence interval from bootstrapping is **(115.6006, 321.9095)**. This means we can say with 95% confidence that the on average the difference between the population means of baby weights from non-smokers and smokers is between 115.6006 and 321.9095.

### **c)**
```{r}
t.test(~Weight | Smoker, data = NCBirths2004, var.equal = FALSE )$conf
```

95% confidence interval from t.test: **(112.3161, 317.6881)**. This means we can say with 95% confidence that the on average the difference between the population means of baby weights from non-smokers and smokers is between 112.3161 and 317.6881.

### **d)**

We can say with 95% confidence that babies born to birth mothers who did not smoke during pregnancy weigh more on average than babies born to birth mothers who did smoke during pregnancy. This is because both 95% confidence intervals calculated for the mean weight of non-smoke - mean weight of smoke in b) and c) is positive.

## **Question 2**

### **a)**
```{r}
q2a_bootstrap = do(2000)*(sd(resample(smoker_vec)) / sd(resample(non_smoker_vec)))
```

### **b)**
```{r}
ggplot(q2a_bootstrap, aes(sample = result))+ stat_qq(size=2, col="blue") + stat_qqline(col="red") + ggtitle("Normal Probability Plot n = 2000 of Bootstrap Stat")
```


Looking at the normal probability plot, the ratio of the sample standard deviations does appear to follow an approx. normal distribution. The majority of the points fall on, or very close to the line.

### **c)**
```{r}
qdata(~result, c(0.025, 0.975), data=q2a_bootstrap)
ggplot(q2a_bootstrap, aes(x = result)) + geom_histogram(fill = "cyan3") + ggtitle("Bootstrapped Ratio of Sample std. Deviations") + geom_vline(xintercept =0.9145372, color = "red") + geom_vline(xintercept = 1.2441046  , color = "red")+ annotate("text", x = 0.9145372, y = -5, label = "Lower Bound", color = "red")+ annotate("text", x = 1.2441046  , y = -5, label = "Upper Bound", color = "red")+ xlab("Ratio of Sample std. Deviation") + ylab("Count")
```

The 95% bootstrap confidence interval is **(0.9145, 1.2441).**  This means we can say with 95% confidence that the on average the ratio  between the standard deviation of baby weights from non-smokers and smokers is between 0.9145 and 1.2441.

### **d)**

A practical result of considering the ratio of sample standard deviation of smoker's baby weights to non-smoker's baby weights would be to see if on average we observe more variability in smoker's baby weights compared to non-smoker's baby weights. Based on our 95% confidence interval calculated in c), there is no difference in the variability of babies weight based on what mother they were born from. This is because the confidence intervals contain 1.


## **Question 3**

```{r}
q3_vec <- c(1.2,1.1,1.0,1.0,1.1,1.0,1.0,1.0,0.9,1.1,1.1,1.2,1.0,1.1,1.0,1.1,1.0,0.9,1.0,1.0,1.1,1.0,1.0,1.1,1.2,1.0,1.1,1.0,1.0,1.2,1.1)
q3_vec1<- rep("ppm", 31)
q3.df <- data.frame(q3_vec1, q3_vec)
```

### **a)**

Action level is set to mercury levels **over** 1 ppm.

Null hypothesis: $H_{0}: \mu = 1.0$

Alternative hypothesis: $H_{A}: \mu > 1.0$


### **b)**

**Type I Error:** Reject $H_{0}$ when $H_{0}$ is true. This would mean concluding that the PPM of mercury in Athabaska River is not less than or equal to 1.0, when it actually is.

**Type II Error:** Fail to reject $H_{0}$ when $H_{0}$ is false. This would mean concluding that the PPM of mercury in Athabaska River is less than or equal to 1.0 when it is actually greater than 1.0.

### **c)**
```{r}
ggplot(q3.df, aes(x = q3_vec, y = q3_vec1)) +  geom_violin(fill = "aquamarine3")+ geom_boxplot(width = 0.3, fill = "darksalmon") + xlab("PPM") + ylab("Athabaska River")
q3.df
```

The PPM of mercury in fish located in the Athabaska River ranged from 0.9 to 1.2 with the most frequent level being 1.0. Most of the data lies above 1.0 PPM rather than below 1.0 PPM.


Because out sample size is greater than 25, we do not need to check for normality before using t.test.


### **d)**
```{r}
t.test(q3_vec, mu = 1, alternative = "greater")
```

Using t.test, we get a p value of 0.0007. Comparing this value to a set alpha value of 0.05, we reject our null hypothesis that the mercury levels are not greater than 1.0 ppm with a signifiance level of 0.05. This means Health Canada should place a moratorium on commercial fishing on walleye fish in Athabaska Rive.

Out p-value means that we have a 0.0007 chance of obtaining test results at least as extreme as the ones we actually observed. This means that it is extremely unlikely that our observation from our sample was a random outlier. 

From our t.test we get a 95% confidence interval of **(1.0269, inf).** The reason the upper bound of the interval is infinity is because we were conducting a right sided test. The meaning of this interval is that we can say with 95% confidence that the mean mercury levels in Athabaska River is greater than 1.0269.

## **Question 4**

We are trying to test if the proportion of environmentally certified coffee growers in Southern Mexico is **greater than 0.6.** We begin testing by setting up the null and alternative hypothesis:

Null hypothesis: $H_{0}: P_{certified} = 0.6$

Alternative hypothesis: $H_{A}: P_{certified} > 0.6$ From this, we know we will be conducting a right tailed test.

From our representative sample of n = 845, we get a test statistic of X = 475 + 75 = 550 environmentally friendly coffee growers. We can now create a distribution of the test statistic by re-sampling n = 845. We will do 1000 re-samples.

```{r}
q4_vec <- c(rep(1, 845*0.6), rep(0, 845*0.4))

q4_samples <- numeric(1000)
for(i in 1:1000){
   pst.simulation <- resample(q4_vec) 
   q4_samples[i] <- sum(pst.simulation) 
}
q4.df <- data.frame(x = q4_samples, below = q4_samples <=845*0.6, above= q4_samples > 845*0.6)
```


Plotting the distribution:

```{r}
ggplot(data=q4.df, aes(x=x, fill = above)) + geom_histogram(binwidth = 1, col = "darksalmon") + ylab("Count")  + geom_vline(xintercept = 507, col="blue") + geom_vline(xintercept = 550, col="blue") + annotate("text", x = 511, y = -5, label = "Null Hypothesis", color = "blue") + annotate("text", x = 548.5, y = -5, label = "X", color = "blue") + theme(legend.position="none")
```

As seen from the distribution, our observed amount of environmentally friendly coffee growers is on the far-right tail, very far from our assumed amount of coffee growers, the null hypothesis, at x = 507. To conclude our test, we will calculate the area under the curve to the right of 550 (X), known as the p-value, and compare this to our alpha value which is set to 0.05. 

```{r}
1 - pbinom(550, 845, 0.60)
```

Using the pbinom function, we get a p-value of 0.0010. Because this is less than our alpha of 0.05, we can reject our null hypothesis and conclude that the proportion of Southern Mexican coffee growers who are either certified or in the process of being certified is more than 60% with probability of type 1 error of 0.05. 

## **Question 5**

To calculate the 95% pivotal quantity confidence interval, we can use prop.test. We will make sure to use the Agretsi-Coull version of the confidence interval by adding 1 to the numerator and 2 to the denominator to each proportion. 

```{r}
prop.test(c(274+1, 348+1), c(376+2, 670+2), conf.level = 0.95, correct=FALSE)$conf
```

The 95% confidence interval is **(0.1495, 0.2668).** This means we can conclude that with 95% confidence that the proportion of people with at least a undergraduate degree that do not believe in the science behind the vaccine is larger than the proportion of people with at most a high school diploma that do not believe in the science behind the vaccine. This is because both our upper and lower bound in our 95% confidence interval of $p_{Uni} - p_{HS}$ is positive.

## **Question 6**

### **a)**

Null hypothesis: $H_{0}: P_{vote} = 0.45$

Alternative hypothesis: $H_{A}: P_{vote} < 0.45$. This tells us that we will be conducting a left sided test.

### **b)**

In order to calculate $\alpha$, we compute the probability $P(Reject H_{0} | H_{0} is True)$. 

$P(H_{0} is True)$ is 0.45.

$P(Reject H_{0})$ is $P(X < 20)$ 

We can use pbinom to calculate alpha. This will give us the probability that we reject the null hypothesis if it actually true.

```{r}
pbinom(19,50, 0.45)
```

We get an alpha value of 0.19737.

### **c)**

To get the probability that we will decide that the candidate should run, we compute the area to the right of x = 20, given that the candidate gets 42% of the votes. 

```{r}
1 - pbinom(19, 50, 0.42)
```

We get a probability of 0.6638.

### **d)**
```{r}
x <- c(0.41, 0.40, 0.39, 0.38, 0.35, 0.30)
y = numeric(6)
for (vals in 1:length(x)){
  y[vals] = 1 - pbinom(19, 50, x[vals])
}
q6.df <- data.frame(x, y)
ggplot(q6.df, aes(x = x, y = y)) + geom_point(stat="identity", color = "blue") + xlab("Proportion") + ylab("Probability")
```

### **e)**

As the proportion of people that vote for the candidate increases, the probability that we conclude that the candidate should run increases as well.

Since we are plotting proportion of people that vote for the candidate that are less than 0.45, these are also beta/type 2 error. ($P(Fail to Reject H_{0} | H_{0} is False)$). One way to improve the test would be to increase the threshold of successes observed in the sample to reject the null hypothesis. This is because we assume the proportion is 0.45, however we advise the candidate to run when we observe at least 20 vote (20/50 = 0.4).

## **Question 7**

### **a)**

To test the claim that the price of used textbooks from the University of Calgary are more than from Amazon, we will create a new variable variable called "Diff" that is price of the used textbook at UofC minus the price at Amazon. We will then do our statistical test on this new variable. The reason we are able to do this is because the "object" (textbook) is the same at both the UofC and Amazon and we can therefore call these two samples dependent. The null and alternative hypothesis are below:

$H_{0}=\mu_{d} = 0$

$H_{A}=\mu_{d} > 0$

```{r}
# Read the data file into R
q7_data <- read.csv("http://people.ucalgary.ca/~jbstall/DataFiles/bookprices.csv")
q7_data["Diff"] = q7_data["UsedBkStore"] - q7_data["UsedAmazon"]
```

```{r}
t.test(q7_data$Diff, mu=0, alternative="greater", data=q7_data)
```


From our t-test, we get a p-value of 0.0044. Using an alpha value of 0.05, means we can reject our null hypothesis that the mean of the difference between used textbook price at UofC and Amazon is 0. So we can conclude that on average, the price of the textbooks sampled are greater at the University of Calgary's bookstore compared to Amazon.


### **b)**

Because both samples have less than 25 observations, we must test that the samples follow an approx. normal distribution using stat_qq.

```{r}
ggplot(q7_data, aes(sample = Diff))+ stat_qq(size=2, col="blue") + stat_qqline(col="red") + ggtitle("Normal Probability Plot n = 15 Difference in Textbook Price")
```

Looking at the  normal probability plot of the difference column, the difference in the textbook prices at UofC and Amazon seem to follow an approx. normal distribution, so we can move on to testing out hypothesis using t.test.

## **Question 8**

We want to test whether the proportion of Albertans with some post-secondary education is different than the claim of the Government of Alberta of 0.62. To start, we will set up our null and alternative hypothesis.

Null hypothesis: $H_{0}: P_{PS} = 0.62$

Alternative hypothesis: $H_{A}: P_{PS} \neq 0.62$. This tells us that we will be conducting a two-sided test.

To conduct a two-sided, we will find the p-value of one side and multiply it by 2. For our test, we will use an alpha of 0.05. We can use binom.test to do this.

We will create a distribution by re-sampling 1000 times and then visualize the distribution. 

```{r}
q8_vec <- c(rep(1, 250*0.62), rep(0, 250*0.32))

q8_samples <- numeric(1000)
for(i in 1:1000){
   pst.simulation <- resample(q8_vec) 
   q8_samples[i] <- sum(pst.simulation) 
}
q8.df <- data.frame(x = q8_samples, below = q8_samples <=250*0.68, above= q8_samples > 250*0.38)
```

```{r}
ggplot(data=q8.df, aes(x=x)) + geom_histogram(binwidth = 1, fill = "aquamarine3", col = "darksalmon") + ylab("Count")  + geom_vline(xintercept = 250*0.65, col="blue") + geom_vline(xintercept = 145, col="blue") + annotate("text", x = (250*0.65)+ 5, y = -5, label = "Null Hypothesis", color = "blue") + annotate("text", x = 146, y = -5, label = "X", color = "blue") + theme(legend.position="none")
```

We will compute the left sided p-value by using two sided binom.test.

```{r}
binom.test(145, 250, 0.62, alternative='two.sided', correct=FALSE)
```

We get a p-value of 0.1932 which is larger than our alpha of 0.05, so we fail to reject the null hypothesis that the proportion of Albertans that have post secondary education is 0.62. Therefore we can conclude that the proportion of Albertans that have post secondary education not is different from 0.62 with a significance level of 0.05. 

## **Question 9**

### **a)**

Since this is a two-sided test, we can use either bootstrapping or prop.test. We will try both and compare the results.


#### **Bootstrapping:**
We first set up to representative vectors of the surveys in 2012 and 2019 respectively so we can sample from them while using bootstrapping. 

```{r}
plastics_2012 <- c(rep(1, 601), rep(0, 1010-601))
plastics_2019 <- c(rep(1, 561), rep(0, 1000-561))

q9_bootstrap = do(1000)*(mean(resample(plastics_2019))- mean(resample(plastics_2012)))
q9_phat.df <- data.frame(q9_bootstrap)
qdata(~result, c(0.025, 0.975), data=q9_phat.df)
ggplot(q9_bootstrap, aes(x = result)) + geom_histogram(fill = "cornflowerblue") + ggtitle("Bootstrapped Sample Proportions") + geom_vline(xintercept = -0.0778, color = "red") + geom_vline(xintercept = 0.0097 , color = "red")+ annotate("text", x = -0.0778, y = -5, label = "Lower Bound", color = "red")+ annotate("text", x = 0.0097 , y = -5, label = "Upper Bound", color = "red")
```

Our 95% confidence interval from bootstrapping is **(-0.0778, 0.0097).**

#### **prop.test using Agretsi-Coull**

```{r}
prop.test(c(561+1, 601+1), c(1000+2, 1010+2), conf.level = 0.95, correct=FALSE)$conf
```

Our 95% confidence interval from prop.test is **(-0.0771, 0.0091).**

Both confidence intervals from bootstrapping and prop.test are very similar.

### **b)**

Since 0 is within our 95% confidence interval, we **cannot** conclude that there is a significant difference between the proportions in 2019 and 2012, with 95% confidence.


## **Question 10**

```{r}
q10_vec <- c(497.2,499.9,495.8,514.2,490.0,498.3,495.1,486.7)
x_bar = mean(q10_vec)
s_sample = sd(q10_vec)
n = length(q10_vec)
q10.df <- data.frame(q10_vec)
```

### **a)**

We can use a statqq plot to check the normality of the data.

```{r}
q10a_bootstrap = do(1000)*(mean(resample(q10_vec)))
q10a.df <- data.frame(q10a_bootstrap)
ggplot(q10a.df, aes(x = result)) + geom_histogram() + xlab("Booststraped Sample Mean")
```

Looking at the plot of bootstrapped sample means of cereal box weights, it seems like the data suggests that Usman is getting around 500g on average in each box. 

### **b)**
```{r}
q10b_bootstrap = do(1000)*((mean(resample(q10_vec))- x_bar)/(sd(resample(q10_vec))/(sqrt(n))))
q10b.df <- data.frame(q10b_bootstrap)
qdata(~result, c(0.05), data=q10b.df)
ggplot(q10b.df, aes(x = result)) + geom_histogram(fill = "chartreuse3") + xlab("Booststrap Statistic") + geom_vline(xintercept = -2.17817 , color = "red")+ annotate("text", x = -1.10, y = -5, label = "5th Percentile", color = "red")
```

The 5th percentile of the distribution of the bootstrap is -2.0121. This means 5% of the distribution lies below -2.0121.

### **c)**
```{r}
x_bar - (-2.1059)*(s_sample/n)
```

We get a 5th percentile of x-bar of 499.30. We cannot infer the same thing as in part a). This is because if the 5th percentile of sample means is 499.30, which is very close to 500g, the weight of each box on average is not going to be 500g since 95% of the sample means will lie above 499.30.

## **Question 11**

### **a)**

To calculate alpha, we calculate $P(Reject H_{0} | H_{0} is True)$. We assume $H_{0}$ is true, so $\beta = 2$. We reject the null hypothesis when $X_{min} < 2.018$

To find this probability, we must find the area under the curve defined by the function $f(x_{min})={ne}^{-n(x_{min}-\beta)})$. The upper down will be 2.018 because this is highest value of $x_{min}$ where we can reject the null hypothesis. The lower bound will be 2 because $\beta \leq x_{min}$, and assuming the null hypothesis is true means beta is 2 and therefore $2 \leq x_{min}$.

Before we integrate the function, we can plug known values of n = 6 and $\beta = 2$ to simplify. We get:

$\int\limits_{2}^{2.018} {6e}^{-6(x_{min}-2)}dx_{min}$.

Simplifying the exponent, we get:

${e}^{12}\int\limits_{2}^{2.018} {6e}^{-6x_{min}}dx_{min}$

It is important to note that ${6e}^{-6x_{min}}$ follows an exponential distribution with $\lambda = 6$, so we can use the function pexp to get the area under the curve.

```{r}
exp(12)*(pexp(2.018, rate = 6) - pexp(2, rate = 6))
```

Multiplying the area under the curve by ${e}^{12}$, we get an alpha value of 0.1024.

### **b)**

This sample does support the null hypothesis. We can only reject the null hypothesis is $X_{min} < 2.018$, however the minimum value in the sample is 2.11.

### **c)**

To get beta, we calculate $P(Fail Reject H_{0} | H_{0} is False)$. A useful note is that this is the compliment of alpha. So we can modify the expression in a) to find probability of concluding $\beta = 2$ when $\beta = 1.8$.

Since out actual beta is 1.8, this is our new lower bound.

$1 - \int\limits_{1.8}^{2.018} {6e}^{-6(x_{min}-2)}dx_{min}$

$1 - ({e}^{(6*1.8)}\int\limits_{2}^{2.018} {6e}^{-6x_{min}}dx_{min})$

We can now use the pexp in R to calculate this.


```{r}
1 - (exp(6*1.8)*(pexp(2.018, rate = 6) - pexp(1.8, rate = 6)))
```

We get a probability of 0.2704 that we will conclude that beta is 2 when beta is actually 1.8.

### **d)**

We must solve for $x_{min}$ in the following expression:

$P(Reject H_{0} | H_{0} is True) = 0.2$

$P(X_{min} < k | \beta = 2) = 0.2$

We can utilize the expression in a).

$\int\limits_{2}^{k} {6e}^{-6(x_{min}-2)}dx_{min} = 0.2$

${e}^{12}\int\limits_{2}^{k} {6e}^{-6x_{min}}dx_{min} = 0.2$

Completing the integration:

${e}^{12}[{-e}^{-6x_{min}}+{e}^{-12}]^k_2=0.2$

Substituting k:

${-e}^{-6k}+{e}^{-12}=\frac{0.2}{{e}^{12}}$

Simplifying:

${0.8e}^{-12}={e}^{-6k}$

Taking the natural log of both sides:

$log(0.8)-12=-6k$

$\frac{log(0.8)-12}{-6}=k$


```{r}
(log(0.8)-12)/(-6)
```

k is approx. 2.0372. This means $\leq X_{min}<2.0372$ would result in a rejection of the null hypothesis.

**Session info:**
```{r}
sessionInfo()
```